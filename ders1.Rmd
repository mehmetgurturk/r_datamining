---
title: "Veri Tipi Belirleme"
author: "Mehmet Gurturk"
date: '2022-03-22'
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Çalışacağımız verisetinin içeri aktarılması için `read.csv()` fonksiyonu kullanılır. 
İçeri aktaracağımız lokal dosya yolu ya da uzak sunucuda tutulan dosyanın yolu fonskiyonuna girdi olarak verilir.

```{r}
veriseti <- read.csv("Fin500.csv")
```

Veriseti yüklendikten sonra verisetinin içeriğini kontrol etmek üzere `head()` ve `tail()` fonsiyonları kullanılır.

```{r}
head(veriseti) 
tail(veriseti)
```

Verisetinin ilk ve son altı (6) satırını gördükten sonra versitesinin yapısına bakmak için `str()` fonksiyonunu ve verisetinin özetini görmek için ise `summary()` fonksiyonunu çalıştırdık.

```{r}
str(veriseti)
summary(veriseti)
```

Verisetimizin içerisindeki `veriseti$Industry`, `veriseti$Inception` ve `veriseti$City` verilerimizi `as.factor()` fonksiyonu ile kendi içlerinde gruplandırdık ve tekrar `<-` komutu ile verilirimizi kendi üzerlerine tekrar atadık.

```{r}
veriseti$Industry <- as.factor(veriseti$Industry)
veriseti$Inception <- as.factor(veriseti$Inception)
veriseti$City <- as.factor(veriseti$City)
```

Verisetimizin içerisinde bazı verilerimizin tipi, verilerin yazılışı uygun değildi. Verilerimizi uygun hale getirmek için öncelikle `gsub()` fonksiyonu ile verilerimizi temizledik. Daha sonra tekrar verilerimizi `<-` komutu ile atadık. Sadece `sub()` komutu ilk sıradaki veriyi değiştirir `gsub()` komutu ise başındaki 'g' yani 'global'den dolayı her argümana bakarak değişim gerçekleştirecek.

Daha sonra Verilerimizin türünü değiştirerek 'numeric' hale dönüştürebilmek için `as.numeric()` fonksiyonunu kullandık. Ve `<-` komutu ile verilirimizi tekrar atama işlemlerini gerçekleştirdik. İşlemleri ilk olarak `veriseti$Expenses` verimize uyguluyoruz.

```{r}
gsub(" Dollars", "", veriseti$Expenses)
gsub(",", "", veriseti$Expenses)
veriseti$Expenses <- as.numeric(veriseti$Expenses)
```

Daha sonra işlemleri `veriseti$Growth` verimize uyguluyoruz.

```{r}
gsub("%", "", veriseti$Growth)
veriseti$Growth <- as.numeric(veriseti$Growth)
```

Son verimizin içerisinde `$` işareti yer almaktadır. Bu işaret R dili için önem arz ettiğinden dolayı bu işareti kaldırırken yanında özel bir işaret daha kullanmamız gerekmektedir. Bunun için `\\$` şeklinde kullanacağız. Ayrıca verilerimizde kullandığımız fonksiyonları iç içe kullanabiliriz.

```{r}
gsub("\\$", "", veriseti$Revenue)
veriseti$Revenue <- as.numeric(gsub(",", "", veriseti$Revenue))
```

Missing Value değerleri ile uğraşacağımzıdan dolayı `library(Amelia)` paketini çağırdık.

```{r}
library(Amelia)
```

Verisetimizin missingness map'ini çizdirebilmek için `missmap()` komutunu uyguladık.

```{r}
missmap(veriseti)
```

Verisetimizin kaç satır ve kaç sütundan meydana geldiğini görebilmek için `dim()` komutunu uyguladık.

```{r}
dim(veriseti)
```

Verisetimizde istediğimiz satır ve sütunları `veriseti[]` komutu uyguluyoruz.

```{r}
veriseti[3,2] #3.satır 2.sütun
veriseti[3, ] #3.satırın tamamı
veriseti[ ,2] #2.satırın tamamı
```

Verisetimizin tam yani NA değer içerip içermediğini görebilmek için `complete.cases()` komutunu uyguluyoruz. Verisetimiz tam yani NA değer yoksa true, NA değer varsa false olarak geri dönüş vermektedir.

```{r}
complete.cases(veriseti)
```

Verisetimizdeki NA değerlerinin yer aldığı satırı göstermek için;

```{r}
veriseti[!complete.cases(veriseti), ] # ! ile incomplete olanları getirdik
```

Verisetimizde geliri 9 milyon olanları getirmek için;

```{r}
veriseti$Revenue == 9746272
```

Verisetimizde geliri 9 milyon olan satırı getirmek için;

```{r}
veriseti[veriseti$Revenue == 9746272, ]
```

True'ların yanında NA'lar dönüyor. Bundan kaçınmak için `which()` komutunu kullanıyoruz.

```{r}
which(veriseti$Revenue == 9746272)
```

Verisetimizde NA değeri olan satırları getirmek için;

```{r}
veriseti[is.na(veriseti$Expenses), ]
```

Verisetimizde çalışan sayısı 45 olan satırı getirmek için;

```{r}
veriseti[which(veriseti$Employees == 45), ]
```

verisetimizi yedeklemek için `<-` komutunu kullandık.

```{r}
backup <- veriseti
```

Verisetimizde complete olanlar ile tekrar veriseti oluşturmak için;

```{r}
veriseti <- veriseti[complete.cases(veriseti), ]
```

Verisetindeki satır numaralarını görmek için `row.names()` komutunu kullandık.

```{r}
row.names(veriseti)
```

Verisetimizde 1'den başlayarak tekrar satır numarası atamak için `1:nrow()` komutunu kullandık.

```{r}
veriseti <- 1:nrow(veriseti)
```

Kayıp verilerin değiştirilmesi için ilk olarak State sütununda NA olan değerleri çağırdık daha sonra da NA verileri değiştirdik.

```{r}
veriseti[!complete.cases(veriseti)]
veriseti[is.na(veriseti$State), ]
veriseti[is.na(veriseti$State) & veriseti$City == "New York", ]
veriseti[is.na(veriseti$State) & veriseti$City == "New York", "State"] <- "NY" #City sütunu New York olan ancak State sütununda NA olan yerlere NY ekledik

veriseti[is.na(veriseti$State) & veriseti$city == "San Francisco"]
veriseti[is.na(veriseti$State) & veriseti$City == "San Francisco", "State"] <- "CA"
```

Verisetimizde NA değeri olan sütunları tespit ettikten sonra "Employees" sütununda yer alan NA değerleri yerine aynı sütundaki meadian değerini atadık. "Employees" sütunu ile "Industry" sütunnda "Retail" olanları atadık. Daha sonra "Industry" sütununda "Fİnancial Services" olanları atadık.

```{r}
summary(veriseti$Employees)
veriseti[is.na(veriseti$Employees), ]

median(veriseti[, "Employees"]) # NA baskın olduğu için sonuç NA çıktı bundan kurultmak için aşağıdaki komutu girdik 
median(veriseti[, "Employees"], na.rm = T) #NA ya median değerini ata NA'yı dikkate alma
med_emp_ret <- median(veriseti[veriseti$Industry == "Retail", "Employees"], na.rm = T)

veriseti[is.na(veriseti$Employees) & veriseti$Industry == "Retail", ]
veriseti[is.na(veriseti$Employees) & veriseti$Industry == "Retail", "Employees"] <- med_emp_ret

med_emp_finservices <- median(veriseti[veriseti$Industry == "Financial Services", "Employees"], na.rm = T) 
veriseti[is.na(veriseti$Employees) & veriseti$Industry == "Financial Services", "Employees"] <- med_emp_finservices
```

"Expenses" kolununda NA olan değerleri "Revenue" kolunu "Profit" kolunu dolan verilerden çıkartarak elde ettik.

```{r}
summary(veriseti)
head(veriseti,25)

veriseti[is.na(veriseti$Expenses), ]
veriseti[is.na(veriseti$Expenses) & !is.na(veriseti$Revenue), ] #Revenue boş olmaması lazım
veriseti[is.na(veriseti$Expenses) & !is.na(veriseti$Revenue), "Expenses" ] #expenses kolonunu verecek

veriseti[is.na(veriseti$Expenses) & !is.na(veriseti$Revenue), "Expenses" ] <- veriseti[is.na(veriseti$Expenses) & !is.na(veriseti$Revenue), "Revenue" ] - veriseti[is.na(veriseti$Expenses) & !is.na(veriseti$Revenue), "Profit" ] 
```

"Revenue" kolununda NA olan değerleri kolunun ortalamasını NA olan değerlere atayarak elde ettik.

```{r}
summary(veriseti)
head(veriseti,25)

veriseti[is.na(veriseti$Revenue), ]
veriseti[is.na(veriseti$Revenue) & veriseti$Industry == "Construction", ] #Revenue NA olan Industry'ler Construction

mean(veriseti[, "Revenue"])
mean(veriseti[, "Revenue"], na.rm = T)
mea_rev <- mean(veriseti[, "Revenue"], na.rm = T)

veriseti[is.na(veriseti$Revenue) & veriseti$Industry == "Construction", "Revenue"] <- mea_rev
```

Matematiksel işlemler ile çalışacağımızdan dolayı `caret` paketini yükledik. Daha sonra R'da yerleşik veri kümesi olan `iris` veri kümesini çağırdık ve `iris` veri kümesinin yapısını ve özetini ortaya koyduk.

```{r}
library(caret)
data(iris)
str(iris)
summary(iris[,1:4]) #verisetindeki tüm satırlar ile 1. ve 4. sütunlar arasını aldık. Eğer 1. 3. ve 5. sütunları almak isteseydik C(1,3,5) komutunu uygulayacaktık.
```

Verisetimizde uygulayacağımız dönüşümler (matematiksel işlemler) için `preProcess` komutunu kullandık ve `preProcess` altındaki `scale` paketini çağırdık.

```{r}
preProcessParams <- preProcess(iris[,1:4], method=c("scale")) #Herbir gözlemi standart sapmaya oranladık. 
print(preProcessParams)
```

$$
\frac{x_(i)}{\sigma_(x)}
$$
İşlemleri belirli veri kümelerine uygulamak için `predict` komutunu kullandık.

```{r}
scaled <- predict(preProcessParams, iris[,1:4]) #predict fonksiyonunu dönüştürerek iris 1:4 e yazdırdık.
summary(scaled)
```

`preProcess` altındaki `scale` paketini çağırdık ve dönüşüm yaparak yazdırdık.

```{r}
preProcessParams <- preProcess(iris[,1:4], method=c("center")) #xi-xmü yaptık
print(preProcessParams)

centered <-  predict(preProcessParams, iris[,1:4])
summary(centered)
```

$$
x_{i}-\bar{X}_{x}
$$
Standardizasyon uygulayabilmek için;

```{r}
preProcessParams <- preProcess(iris[,1:4], method=c("center", "scale"))
print(preProcessParams)

standardized <- predict(preProcessParams, iris[,1:4])
summary(standardized)
```

$$
\frac{x_{i}-\mu{x}}{\sigma_{x}}
$$

Normalizasyon uygulayabilmek için;

```{r}
preProcessParams <- preProcess(iris[,1:4], method=c("range")) 
print(preProcessParams)

normalized <- predict(preProcessParams, iris[,1:4])
summary(normalized)
```

$$
\frac{x-min\left(x \right )}{max\left(x \right )-min\left(x \right )}
$$
`boxcox` dönüşümü, normal olarak dağıtılmamış bir veri kümesini daha normal dağılmış bir veri kümesine dönüştürmek için yaygın olarak kullanılan bir yöntemdir. `boxcox` uygulayabilmek için;

```{r}
preProcessParams <- preProcess(iris[,1:4], method=c("BoxCox"))
print(preProcessParams)

boxcox <- predict(preProcessParams, iris[,1:4])
summary(boxcox)
```

$$
y\left( \lambda \right )=\left\{\begin{matrix}\frac{y^ \lambda -1 }{\lambda}, & if \lambda\neq 0\\ log\left (y  \right ), & if \lambda = 0 \end{matrix}\right.
$$

`Yeo-Johnson` dönüşümü, normalleştirme dönüşümünü hesaplar. `yeojohnson` uygulabilmek için;

```{r}
preProcessParams <- preProcess(iris[,1:4], method=c("YeoJohnson"))
print(preProcessParams)

yeojohnson <- predict(preProcessParams, iris[,1:4])
summary(yeojohnson)
```

$$
\psi \left( \lambda, y \right )=

\left\{\begin{matrix}

({(\lambda +1)^\lambda -1)/\lambda}, & if \lambda\neq 0, y\geq 0\\

log\left (y+1  \right ), & if \lambda = 0, y\geq 0 \\ 

-[(-y+1)^{2-\lambda }-1)]/(2-\lambda), & if \lambda \neq  2, y< 0 \\ 

-log\left (-y+1  \right ), & if \lambda = 2, y< 0\end{matrix}\right.
$$

Teniden örnekleme yöntemi için ilk olarak veri setimiz geri çağırdık. Bunun için;

```{r}
data(iris)
summary(dataset)
dataset <- iris
```

Bootstrap ile 100 örneklem oluşturduk. Bu örneklemin oluşturulması için  yapılması gereken işlemler;

```{r}
train_control <- trainControl(method="boot", number=100)
```

Verisetinin %80'ini ayıran komut işlemi için;

```{r}
split=0.80
trainIndex <- createDataPartition(iris$Species, p=split, list=FALSE)
data_train <- iris[ trainIndex, ]
data_test <- iris[ -trainIndex, ]
```

Leave One Out Cross Validation yöntemiyle örnekleme yapmak için ilk caret package'ı aktif ettik. Sonrasında örneklem yönteminin komutu yazdık. Bu adımlar aşağıdaki gibi gösterilebilir.

```{r}
Library(caret)
data(iris)
train_control <-trainControl(method="LOOCV")
```

K-Fold Cross Validation için aşağıdaki adımları yürüttük;

```{r}
data(iris)
train_control <- trainControl(method="cv", number=10)
```

Son olarak Repeated Cross Validations yöntemini uygulamak içinse aşağıdaki adımları uyguladıkİ

```{r}
data(iris)
train_control <- trainControl (method="repeatedcv", number=10, repeats=3)
```

`cars` paketinin ilk altı (6) satırını, yapısını ve özetine baktık.

```{r}
head(cars)
str(cars)
summary(cars)
```

Daha sonra değişkenleri isimleri ile çağıracağımızdan dolayı `names()` fonksiyonunu kullandık.

```{r}
names(cars)
```

Verisetinin regresyon modeli uygun olup olmadığını ile eğrisel mi yoksa doğrusal mı olduğunu `scatter.smooth()` kodu ile gösterdik.

```{r}
scatter.smooth(x=cars$speed, y=cars$dist, main="Saçılma Diyagramı")
```

Korelasyon üretmek için `cor()` kodunu kullandık.

```{r}
cor(cars$speed, cars$dist)
```

Regresyon modelini oluşturmak için `lm()` fonksiyonunu kullandık ve bunu 'GenelModel'e atadık.

```{r}
GenelModel <- lm(dist ~ speed, data=cars) # lm(bağımlı ve bağımsız değişken) (lm, bağımlı değişken, bağımsız değişken , veriseti)
```

GenelModel içerisindeki tahmini 'distance'ı almak için `print()` ve daha sonra GenelModel'in özetini görmek için `summary()` komutunu kullandık.

```{r}
print(GenelModel)
summary(GenelModel)
```

Bilgi kriterlerini bulabilmek için `AIC()` ve `BIC()` komutlarını kullandık.

```{r}
AIC(GenelModel) 
BIC(GenelModel)
```

Rassal değerleri bulabilmek için `set.seed()` fonksiyonundan yararlandık.

```{r}
set.seed(100)
```

Satır index değerlerini `sample()` fonksiyonu ile oluşturduk ve değerleri 'trainingRowIndex'e atadık.

```{r}
trainingRowIndex <- sample(1:nrow(cars), 0.8*nrow(cars)) # 1'den n row'a kadar verinin yüzde 80'ini rassal olarak seçtik.
```

cars verisetindeki daha önce oluşturduğumuz trainingRowIndex satırını ve tüm sütununu 'trainingData'ya atadık.

```{r}
trainingData <- cars[trainingRowIndex, ] #satırlarını al ve tüm sütunlarını al
```

'testData'ya 'trainingRowIndex'in tersini atadık.

```{r}
testData <- cars[-trainingRowIndex, ]
```

Tekrar Regresyon modelini oluşturmak için `lm()` fonksiyonunu kullandık ve bunu 'lmMod'a atadık. Daha sonra testData'sının tahmini değerlerini `predict()` komutu ile aldık ve `print()` `summary()` ile `ÀIC()` komutları ile verisinin içerisine baktık.

```{r}
lmMod <- lm(dist ~ speed, data=trainingData)
distPred <- predict(lmMod, testData) 
print(distPred)
summary(lmMod)
AIC(lmMod)
```

Gözlemlediğimiz ve tahmin ettiğimiz verileri birleştiriyor (bind). Yani iki vektörü birleştiriyor (bind) ve yeni bir vektör oluşturuyor. Bunun için `data.frame()` fonksiyonundan yararlandık. Ve yeni bulduğumuz vektörü 'actuals_preds'e atadık.

```{r}
actuals_preds <- data.frame(cbind(gercek=testData$dist, tahmin=distPred)) 
```

Yeni oluşturduğumuz verimizin korelasyonuna `cor()` fonksiyonu ile bakıyoruz ve bunu 'correlation_accuracy'e atadık.Daha sonra verimizin ilk altı (6) satırına baktık.

```{r}
correlation_accuracy <- cor(actuals_preds)
head(actuals_preds)
```

mape (min personch error) yani gerçek değer ie tahmini değer arasındaki farkın mutlak değerini alarak gerçek değere oranlıyoruz. Diğer bir deyişle hata payını alarak gerçek değere oranlıyoruz. Ve `print()` komutu ile çıktısını alıyoruz.

```{r}
mape <- mean(abs(actuals_preds$tahmin - actuals_preds$gercek)/actuals_preds$gercek)
print(mape)
```

Kütüphaneden "Data Analysis and Graphics Using R" `DAAG` paketini çağırdık. `suppressWarnings` komutu ile tüm uyarıları yok saydık ve 5 tane regresyon modeli oluşturarak makina öğrenmesi gerçekleştirdik. `attr` ile br nesnenin belirli niteliklerini aldık.

```{r}
library(DAAG)
cvResults <- suppressWarnings(CVlm(cars, form.lm = dist ~ speed, m=5, dots = FALSE, seed = 29, legend.pos = "topleft", printit = FALSE, main = "CV Dogrusal Regresyon"));
#cvelm ile crossvalidantianlm çağırıldı. cars kullanılacak veristesini tanımladık, formülü tanımlayarak 5 grup oluşturduk, dots ile noktaları göstermedik, 5 grupda da aynı değerleri bulmak için seed'i belirledik, legend.pos ile gösterileceği yeri belirledik, 
attr(cvResults, 'ms') # ms=squaared error
``` 

`tidyverse` paketi veri bilimi iş akışından veri görselleştirmeye kadar tüm paketleri içerir. `caret` (Classification and Regression Training) paketi karmaşık regresyon ve sınıflandırma sorunları düzelten ve düzene sokan bir pakettir. `glmnet` paketi  lasso or elastic-net regularization, linear regression, logistic and multinomial regression models, Poisson regression, Cox model, multiple-response Gaussian, and the grouped multinomial regression oluşturmak için uygundur.

```{r}
library(tidyverse)
library(caret)
library(glmnet)
data(Boston, package = "MASS")
```

Rastgele fakat sabit verimizi `set.seed` komutu ile oluşturduk.

```{r}
set.seed(1212)
```

Test ve eğitim veri setlerimizi oluşturduk.

```{r}
sample_size <- floor(0.75 * nrow(Boston)) #floor komutu ile sayıları yuvarlıyoruz.
training_index <- sample(seq_len(nrow(Boston)), size = sample_size)
train <- Boston[training_index, ]
test <- Boston[-training_index, ] #training_index değerleri dışındaki değerleri al anlamına gelmektedir.
```

Modelimizin tahmini ve nihai sonucunu elde etmek için;

```{r}
#Predictor
x <- model.matrix(medv~., train)[,-1] #Bağımlı değişkenimiz medv. Diğerleri bağımsız değişkenlerimiz. -1 ile medv value kolununu işlemden çıkartıyoruz. Diğer bir deyişle gözlem değeri kullanılmayacağı için çıkartıyoruz.
#Response
y <- train$medv
```

Ridge Regresyonu bulabilmek için `àlpha=0` değeri olması gerekmektedir.

```{r}
cv.r <- cv.glmnet(x, y, alpha = 0) #İdeal fonksiyonu ürettik
cv.r$lambda.min
model.ridge <- glmnet(x, y, alpha=0, lambda =  cv.r$lambda.min)
coef(model.ridge) #`coef` komutu ile coeffecient kat sayılarıana baktık

x.test.ridge <- model.matrix(medv~., test)[,-1] #Modelimiz ile ridge'e atadık 
predictions.ridge <- model.ridge %>% predict(x.test.ridge) %>% as.vector() #Modelin predict değerini aldık ve vektör olarak yazdırdık 
data.frame( RMSE.r = RMSE(predictions.ridge, test$medv), Rsquare.r = R2(predictions.ridge, test$medv)) #Predictions ridge yani tahmini ridge değerlerini bulduk
```

Lasso Regresyonunu bulabilmek için `àlpha=1` değeri olması gerekmektedir.

```{r}
cv.l <- cv.glmnet(x, y, alpha=1) #İdeal fonksiyonu ürettik
cv.l$lambda.min
model.lasso <- glmnet(x, y, alpha=1, lambda =  cv.l$lambda.min)
coef(model.lasso) #`coef` komutu ile coeffecient kat sayılarıana baktık

x.test.lasso <- model.matrix(medv~., test)[,-1] #Modelimiz ile ridge'e atadık 
predictions.lasso <- model.lasso %>% predict(x.test.lasso) %>% as.vector() #Modelin predict değerini aldık ve vektör olarak yazdırdık 
data.frame( RMSE.l = RMSE(predictions.lasso, test$medv), Rsquare.l = R2(predictions.lasso, test$medv)) #Predictions lasso yani tahmini lasso değerlerini bulduk
```

Elastic Net Regresyonu bulabilmek için;

```{r}
model.net <- train(medv~., data = train, method ="glmnet", trControl = trainControl("cv", number=10), tuneLength=10)
model.net$bestTune
coef(model.net$finalModel, model.net$bestTune$lambda) #`coef` komutu ile coeffecient kat sayılarıana baktık

x.test.net <- model.matrix(medv~., test)[,-1] #Modelimiz ile ridge'e atadık 
predictions.net <- model.net %>% predict(x.test.net) #Modelin predict değerini aldık 
data.frame(RMSE.net=RMSE(predictions.net, test$medv), Rsquare.net=R2(predictions.net, test$medv)) #Predictions elastic net yani tahmini elastic net değerlerini bulduk
```

Geleneksel model ile tahmini modeli bulabilmek için;

```{r}
dataset = read.csv("Mortgage.csv")
head(dataset) #datasetin ilk değerlerini gördük
str(dataset) #dataset'in yapısına baktık
dataset$y = as.factor(dataset$y)

model1 = glm(y~x1+x2, family = "binomial", data = dataset)
summary(model1) #model1'in özetini kontrol ettik

#Modelimizi test ettik.
ll.null = model1$null.deviance/-2
ll.proposed = model1$deviance/-2

R2 = (ll.null - ll.proposed)/ ll.null #Hataların toplamını yani en uygun durumu ifade eder.

#Kendi modelimizi test ettik. Şimdi de kendi modelimizi tahmin edeceğiz.

tahmin = predict(model1, type = "response")
classification = round(tahmin)

AR  = mean(dataset$y == classification)

#Geleneksel model ile tahmin modelimizi bulduk.

```

`Spam.csv` veri setimizin tahmini modelini bulacağız.

```{r}
dataset = read.csv("Spam.csv")
dataset$Spam = as.factor(dataset$Spam)
str(dataset) #dataset'in yapısına baktık

set.seed(1453) #Rastgele fakat sabit verimizi `set.seed` komutu ile oluşturduk.
trainingRowIndex = sample(1:nrow(dataset), 0,8*nrow(dataset))
train = dataset[trainingRowIndex,2:5] #Record kolonu hariç diğer kolanları aldık
test = dataset[-trainingRowIndex, 2:5] #training_index değerleri dışındaki değerleri aldık.

model2 = glm(Spam~., family = "binomial", data = train)
summary(model2) #model1'in özetini kontrol ettik

#Modelimizi test ettik.
ll.null = model2$null.deviance/-2
ll.proposed = model2$deviance/-2

#Hataların toplamını yani en uygun durumu ifade eder.
R2 = (ll.null - ll.proposed)/ ll.null

#Kendi modelimizi test ettik. Şimdi de kendi modelimizi tahmin edeceğiz.
tahmin2 = predict(model2, test, type="response")
classification = round(tahmin2)

#Tahmin modelimizi bulduk.
AR = 100*mean(test$Spam == classification)
```

'traintestdata'sını crossvalidation ile kaç kümeyi bölündüğünü bulacağız.

```{r}
library(caret)
dataset = read.csv("Spam.csv")
dataset = dataset[ ,2:5]
dataset$Spam = as.factor(dataset$Spam)
str(dataset)

myControl = trainControl(method = "cv", number = 5) #verisetini 5'e bölerek crossvalidation yapıyoruz.

model3 = train(Spam~., data = dataset, trControl=myControl, method="glm", family=binomial, metric="Accuracy") #crossvalidation'ın rassal olarak etkinlik değerlerini bulduk.

model3

```

